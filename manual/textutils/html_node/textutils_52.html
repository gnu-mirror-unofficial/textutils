<HTML>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- Created on December, 28  2000 by texi2html 1.64 -->
<!-- 
Written by: Lionel Cons <Lionel.Cons@cern.ch> (original author)
            Karl Berry  <karl@freefriends.org>
            Olaf Bachmann <obachman@mathematik.uni-kl.de>
            and many others.
Maintained by: Olaf Bachmann <obachman@mathematik.uni-kl.de>
Send bugs and suggestions to <texi2html@mathematik.uni-kl.de>
 
-->
<HEAD>
<TITLE>GNU text utilities: Putting the tools together</TITLE>

<META NAME="description" CONTENT="GNU text utilities: Putting the tools together">
<META NAME="keywords" CONTENT="GNU text utilities: Putting the tools together">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<META NAME="Generator" CONTENT="texi2html 1.64">

</HEAD>

<BODY LANG="EN" BGCOLOR="#FFFFFF" TEXT="#000000" LINK="#0000FF" VLINK="#800080" ALINK="#FF0000">

<A NAME="SEC52"></A>
<TABLE CELLPADDING=1 CELLSPACING=1 BORDER=0>
<TR><TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_51.html#SEC51"> &lt; </A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_53.html#SEC53"> &gt; </A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT">[ &lt;&lt; ]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils.html#SEC_Top"> Up </A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[ &gt;&gt; ]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils.html#SEC_Top">Top</A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_toc.html#SEC_Contents">Contents</A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_53.html#SEC53">Index</A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_abt.html#SEC_About"> ? </A>]</TD>
</TR></TABLE>
<HR SIZE=1>
<H2> Putting the tools together </H2>
<!--docid::SEC52::-->
<P>

Now, let's suppose this is a large BBS system with dozens of users
logged in.  The management wants the SysOp to write a program that will
generate a sorted list of logged in users.  Furthermore, even if a user
is logged in multiple times, his or her name should only show up in the
output once.
</P><P>

The SysOp could sit down with the system documentation and write a C
program that did this. It would take perhaps a couple of hundred lines
of code and about two hours to write it, test it, and debug it.
However, knowing the software toolbox, the SysOp can instead start out
by generating just a list of logged on users:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ who | cut -c1-8
arnold
miriam
bill
arnold
</pre></td></tr></table></P><P>

Next, sort the list:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ who | cut -c1-8 | sort
arnold
arnold
bill
miriam
</pre></td></tr></table></P><P>

Finally, run the sorted list through <CODE>uniq</CODE>, to weed out duplicates:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ who | cut -c1-8 | sort | uniq
arnold
bill
miriam
</pre></td></tr></table></P><P>

The <CODE>sort</CODE> command actually has a <SAMP>`-u'</SAMP> option that does what
<CODE>uniq</CODE> does. However, <CODE>uniq</CODE> has other uses for which one
cannot substitute <SAMP>`sort -u'</SAMP>.
</P><P>

The SysOp puts this pipeline into a shell script, and makes it available for
all the users on the system:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre># cat &#62; /usr/local/bin/listusers
who | cut -c1-8 | sort | uniq
^D
# chmod +x /usr/local/bin/listusers
</pre></td></tr></table></P><P>

There are four major points to note here.  First, with just four
programs, on one command line, the SysOp was able to save about two
hours worth of work.  Furthermore, the shell pipeline is just about as
efficient as the C program would be, and it is much more efficient in
terms of programmer time.  People time is much more expensive than
computer time, and in our modern "there's never enough time to do
everything" society, saving two hours of programmer time is no mean
feat.
</P><P>

Second, it is also important to emphasize that with the
<EM>combination</EM> of the tools, it is possible to do a special
purpose job never imagined by the authors of the individual programs.
</P><P>

Third, it is also valuable to build up your pipeline in stages, as we did here.
This allows you to view the data at each stage in the pipeline, which helps
you acquire the confidence that you are indeed using these tools correctly.
</P><P>

Finally, by bundling the pipeline in a shell script, other users can use
your command, without having to remember the fancy plumbing you set up for
them. In terms of how you run them, shell scripts and compiled programs are
indistinguishable.
</P><P>

After the previous warm-up exercise, we'll look at two additional, more
complicated pipelines.  For them, we need to introduce two more tools.
</P><P>

The first is the <CODE>tr</CODE> command, which stands for "transliterate."
The <CODE>tr</CODE> command works on a character-by-character basis, changing
characters. Normally it is used for things like mapping upper case to
lower case:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ echo ThIs ExAmPlE HaS MIXED case! | tr '[A-Z]' '[a-z]'
this example has mixed case!
</pre></td></tr></table></P><P>

There are several options of interest:
</P><P>

<DL COMPACT>
<DT><SAMP>`-c'</SAMP>
<DD>work on the complement of the listed characters, i.e.,
operations apply to characters not in the given set
<P>

<DT><SAMP>`-d'</SAMP>
<DD>delete characters in the first set from the output
<P>

<DT><SAMP>`-s'</SAMP>
<DD>squeeze repeated characters in the output into just one character.
</DL>
<P>

We will be using all three options in a moment.
</P><P>

The other command we'll look at is <CODE>comm</CODE>.  The <CODE>comm</CODE>
command takes two sorted input files as input data, and prints out the
files' lines in three columns.  The output columns are the data lines
unique to the first file, the data lines unique to the second file, and
the data lines that are common to both.  The <SAMP>`-1'</SAMP>, <SAMP>`-2'</SAMP>, and
<SAMP>`-3'</SAMP> command line options omit the respective columns. (This is
non-intuitive and takes a little getting used to.)  For example:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ cat f1
11111
22222
33333
44444
$ cat f2
00000
22222
33333
55555
$ comm f1 f2
        00000
11111
                22222
                33333
44444
        55555
</pre></td></tr></table></P><P>

The single dash as a filename tells <CODE>comm</CODE> to read standard input
instead of a regular file.
</P><P>

Now we're ready to build a fancy pipeline.  The first application is a word
frequency counter.  This helps an author determine if he or she is over-using
certain words.
</P><P>

The first step is to change the case of all the letters in our input file
to one case.  "The" and "the" are the same word when doing counting.
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ tr '[A-Z]' '[a-z]' &#60; whats.gnu | ...
</pre></td></tr></table></P><P>

The next step is to get rid of punctuation.  Quoted words and unquoted words
should be treated identically; it's easiest to just get the punctuation out of
the way.
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ tr '[A-Z]' '[a-z]' &#60; whats.gnu | tr -cd '[A-Za-z0-9_ \012]' | ...
</pre></td></tr></table></P><P>

The second <CODE>tr</CODE> command operates on the complement of the listed
characters, which are all the letters, the digits, the underscore, and
the blank.  The <SAMP>`\012'</SAMP> represents the newline character; it has to
be left alone.  (The ASCII tab character should also be included for
good measure in a production script.)
</P><P>

At this point, we have data consisting of words separated by blank space.
The words only contain alphanumeric characters (and the underscore).  The
next step is break the data apart so that we have one word per line. This
makes the counting operation much easier, as we will see shortly.
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ tr '[A-Z]' '[a-z]' &#60; whats.gnu | tr -cd '[A-Za-z0-9_ \012]' |
&#62; tr -s '[ ]' '\012' | ...
</pre></td></tr></table></P><P>

This command turns blanks into newlines.  The <SAMP>`-s'</SAMP> option squeezes
multiple newline characters in the output into just one.  This helps us
avoid blank lines. (The <SAMP>`&#62;'</SAMP> is the shell's "secondary prompt."
This is what the shell prints when it notices you haven't finished
typing in all of a command.)
</P><P>

We now have data consisting of one word per line, no punctuation, all one
case.  We're ready to count each word:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ tr '[A-Z]' '[a-z]' &#60; whats.gnu | tr -cd '[A-Za-z0-9_ \012]' |
&#62; tr -s '[ ]' '\012' | sort | uniq -c | ...
</pre></td></tr></table></P><P>

At this point, the data might look something like this:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>  60 a
   2 able
   6 about
   1 above
   2 accomplish
   1 acquire
   1 actually
   2 additional
</pre></td></tr></table></P><P>

The output is sorted by word, not by count!  What we want is the most
frequently used words first.  Fortunately, this is easy to accomplish,
with the help of two more <CODE>sort</CODE> options:
</P><P>

<DL COMPACT>
<DT><SAMP>`-n'</SAMP>
<DD>do a numeric sort, not a textual one
<P>

<DT><SAMP>`-r'</SAMP>
<DD>reverse the order of the sort
</DL>
<P>

The final pipeline looks like this:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ tr '[A-Z]' '[a-z]' &#60; whats.gnu | tr -cd '[A-Za-z0-9_ \012]' |
&#62; tr -s '[ ]' '\012' | sort | uniq -c | sort -nr
 156 the
  60 a
  58 to
  51 of
  51 and
 ...
</pre></td></tr></table></P><P>

Whew!  That's a lot to digest.  Yet, the same principles apply. With six
commands, on two lines (really one long one split for convenience), we've
created a program that does something interesting and useful, in much
less time than we could have written a C program to do the same thing.
</P><P>

A minor modification to the above pipeline can give us a simple spelling
checker!  To determine if you've spelled a word correctly, all you have to
do is look it up in a dictionary.  If it is not there, then chances are
that your spelling is incorrect.  So, we need a dictionary.  If you
have the Slackware Linux distribution, you have the file
<TT>`/usr/lib/ispell/ispell.words'</TT>, which is a sorted, 38,400 word
dictionary.
</P><P>

Now, how to compare our file with the dictionary?  As before, we generate
a sorted list of words, one per line:
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ tr '[A-Z]' '[a-z]' &#60; whats.gnu | tr -cd '[A-Za-z0-9_ \012]' |
&#62; tr -s '[ ]' '\012' | sort -u | ...
</pre></td></tr></table></P><P>

Now, all we need is a list of words that are <EM>not</EM> in the
dictionary.  Here is where the <CODE>comm</CODE> command comes in.
</P><P>

<TABLE><tr><td>&nbsp;</td><td class=example><pre>$ tr '[A-Z]' '[a-z]' &#60; whats.gnu | tr -cd '[A-Za-z0-9_ \012]' |
&#62; tr -s '[ ]' '\012' | sort -u |
&#62; comm -23 - /usr/lib/ispell/ispell.words
</pre></td></tr></table></P><P>

The <SAMP>`-2'</SAMP> and <SAMP>`-3'</SAMP> options eliminate lines that are only in the
dictionary (the second file), and lines that are in both files.  Lines
only in the first file (standard input, our stream of words), are
words that are not in the dictionary.  These are likely candidates for
spelling errors.  This pipeline was the first cut at a production
spelling checker on Unix.
</P><P>

There are some other tools that deserve brief mention.
</P><P>

<DL COMPACT>
<DT><CODE>grep</CODE>
<DD>search files for text that matches a regular expression
<P>

<DT><CODE>egrep</CODE>
<DD>like <CODE>grep</CODE>, but with more powerful regular expressions
<P>

<DT><CODE>wc</CODE>
<DD>count lines, words, characters
<P>

<DT><CODE>tee</CODE>
<DD>a T-fitting for data pipes, copies data to files and to standard output
<P>

<DT><CODE>sed</CODE>
<DD>the stream editor, an advanced tool
<P>

<DT><CODE>awk</CODE>
<DD>a data manipulation language, another advanced tool
</DL>
<P>

The software tools philosophy also espoused the following bit of
advice: "Let someone else do the hard part." This means, take
something that gives you most of what you need, and then massage it the
rest of the way until it's in the form that you want.
</P><P>

To summarize:
</P><P>

<OL>
<LI>
Each program should do one thing well. No more, no less.
<P>

<LI>
Combining programs with appropriate plumbing leads to results where
the whole is greater than the sum of the parts.  It also leads to novel
uses of programs that the authors might never have imagined.
<P>

<LI>
Programs should never print extraneous header or trailer data, since these
could get sent on down a pipeline. (A point we didn't mention earlier.)
<P>

<LI>
Let someone else do the hard part.
<P>

<LI>
Know your toolbox! Use each program appropriately. If you don't have an
appropriate tool, build one.
</OL>
<P>

As of this writing, all the programs we've discussed are available via
anonymous <CODE>ftp</CODE> from <CODE>prep.ai.mit.edu</CODE> as
<TT>`/pub/gnu/textutils-1.9.tar.gz'</TT>.<A NAME="DOCF1" HREF="textutils_fot.html#FOOT1">(1)</A>
</P><P>

None of what I have presented in this column is new. The Software Tools
philosophy was first introduced in the book <CITE>Software Tools</CITE>,
by Brian Kernighan and P.J. Plauger (Addison-Wesley, ISBN
0-201-03669-X).   This book showed how to write and use software
tools.   It was written in 1976, using a preprocessor for FORTRAN named
<CODE>ratfor</CODE> (RATional FORtran).  At the time, C was not as ubiquitous
as it is now; FORTRAN was.  The last chapter presented a <CODE>ratfor</CODE>
to FORTRAN processor, written in <CODE>ratfor</CODE>. <CODE>ratfor</CODE> looks an
awful lot like C; if you know C, you won't have any problem following
the code.
</P><P>

In 1981, the book was updated and made available as <CITE>Software
Tools in Pascal</CITE> (Addison-Wesley, ISBN 0-201-10342-7).  Both books
remain in print, and are well worth reading if you're a programmer.
They certainly made a major change in how I view programming.
</P><P>

Initially, the programs in both books were available (on 9-track tape)
from Addison-Wesley.  Unfortunately, this is no longer the case,
although you might be able to find copies floating around the Internet.
For a number of years, there was an active Software Tools Users Group,
whose members had ported the original <CODE>ratfor</CODE> programs to essentially
every computer system with a FORTRAN compiler.  The popularity of the
group waned in the middle '80s as Unix began to spread beyond universities.
</P><P>

With the current proliferation of GNU code and other clones of Unix programs,
these programs now receive little attention; modern C versions are
much more efficient and do more than these programs do.  Nevertheless, as
exposition of good programming style, and evangelism for a still-valuable
philosophy, these books are unparalleled, and I recommend them highly.
</P><P>

Acknowledgment: I would like to express my gratitude to Brian Kernighan
of Bell Labs, the original Software Toolsmith, for reviewing this column.
</P><P>

<A NAME="Index"></A>
<HR SIZE=1>
<TABLE CELLPADDING=1 CELLSPACING=1 BORDER=0>
<TR><TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_51.html#SEC51"> &lt; </A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_53.html#SEC53"> &gt; </A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT">[ &lt;&lt; ]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils.html#SEC_Top"> Up </A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[ &gt;&gt; ]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT"> &nbsp; <TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils.html#SEC_Top">Top</A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_toc.html#SEC_Contents">Contents</A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_53.html#SEC53">Index</A>]</TD>
<TD VALIGN="MIDDLE" ALIGN="LEFT">[<A HREF="textutils_abt.html#SEC_About"> ? </A>]</TD>
</TR></TABLE>
<BR>  
<FONT SIZE="-1">
This document was generated
by <I>Paul Visscher</I> on <I>December, 28  2000</I>
using <A HREF="http://www.mathematik.uni-kl.de/~obachman/Texi2html
"><I>texi2html</I></A>

</BODY>
</HTML>
